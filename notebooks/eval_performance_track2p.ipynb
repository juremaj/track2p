{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import zscore\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.colors as mcolors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics={}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load track2p folder and suite2p datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all the datasets\n",
    "def find_t2p_directories(parent_directory):\n",
    "    # List all items in the parent directory\n",
    "    items = os.listdir(parent_directory)\n",
    "    # Filter the list to include only directories that start with \"t2p\"\n",
    "    t2p_directories = [item for item in items if item.startswith('t2p') and os.path.isdir(os.path.join(parent_directory, item))]\n",
    "    return t2p_directories\n",
    "\n",
    "parent_directory = '/Users/manonmantez/Desktop/jm038/' \n",
    "t2p_dirs = find_t2p_directories(parent_directory)\n",
    "print(t2p_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 't2p_rigid_chan1_all_'\n",
    "print(path)\n",
    "plane = 'plane0' # which plane to process (the example dataset is single-plane)\n",
    "option= 1 #1: Registration done of all days, 2 : Registration done of two days , 3 : Registration done of three days\n",
    "print(option)\n",
    "days = ['P8', 'P9', 'P10' ,'P11', 'P12','P13','P14']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if option == 2 :\n",
    "    second_day= 'P9' # change this !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    if second_day in days:\n",
    "        index_of_second_day = days.index(second_day)\n",
    "        print(index_of_second_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if option == 3 :\n",
    "    second_day= 'P13' # change this\n",
    "    third_day= 'P14' # change this\n",
    "    if third_day in days:\n",
    "        index_of_second_day = days.index(second_day)\n",
    "        index_of_third_day = days.index(third_day)\n",
    "        print(index_of_second_day)\n",
    "        print(index_of_third_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2p_save_path = os.path.join(parent_directory, path)\n",
    "print(t2p_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.load() the match matrix (plane0_match_mat.npy)\n",
    "t2p_match_mat = np.load(os.path.join(t2p_save_path,  'track2p',f'{plane}_match_mat.npy'), allow_pickle=True)\n",
    "# np.load() settings (this contains suite2p paths etc.) (track_ops.npy)\n",
    "track_ops_dict = np.load(os.path.join(t2p_save_path, 'track2p', 'track_ops.npy'), allow_pickle=True).item()\n",
    "track_ops = SimpleNamespace(**track_ops_dict) # create dummy object from the track_ops dictionary\n",
    "print(track_ops.reg_chan)\n",
    "print(track_ops.transform_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Datasets used for t2p:\\n')\n",
    "for ds_path in track_ops.all_ds_path:\n",
    "    print(ds_path)\n",
    "\n",
    "print('\\nMatch matrix shape:')\n",
    "print(t2p_match_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Above is the output of the algorithm, now generate ground truth\n",
    "### Generate grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the suite2p output of the first dataset\n",
    "s2p_path = os.path.join(track_ops.all_ds_path[0], 'suite2p', plane)\n",
    "\n",
    "# load the ops and stat\n",
    "ops_first = np.load(os.path.join(s2p_path, 'ops.npy'), allow_pickle=True).item()\n",
    "stat_first = np.load(os.path.join(s2p_path, 'stat.npy'), allow_pickle=True)\n",
    "iscell_first = np.load(os.path.join(s2p_path, 'iscell.npy'), allow_pickle=True)\n",
    "s2p_inds = np.arange(len(stat_first))\n",
    "\n",
    "# filter stat based on the track2p probability\n",
    "print(f'Filtering stat based on the track2p probability (iscell_thr={track_ops.iscell_thr})')\n",
    "s2p_inds_iscell = s2p_inds[iscell_first[:, 0] > track_ops.iscell_thr]\n",
    "stat_first_iscell = stat_first[iscell_first[:, 0] > track_ops.iscell_thr]\n",
    "print(f'Filtered stat from {len(stat_first)} to {len(stat_first_iscell)} cells')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plot the fov with contours\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(ops_first['meanImg'], cmap='gray')\n",
    "for i in range(len(stat_first_iscell)):\n",
    "    # get random color \n",
    "    color = np.random.rand(3)\n",
    "    plt.scatter(stat_first_iscell[i]['xpix'], stat_first_iscell[i]['ypix'], s=1, color=color, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now make a 8x8 grid on the image\n",
    "n_grid = 9\n",
    "grid_x = np.arange(0, ops_first['meanImg'].shape[1], ops_first['meanImg'].shape[1] // n_grid)\n",
    "grid_y = np.arange(0, ops_first['meanImg'].shape[0], ops_first['meanImg'].shape[0] // n_grid)\n",
    "\n",
    "plt.imshow(ops_first['meanImg'], cmap='gray')\n",
    "for i in range(n_grid):\n",
    "    plt.axvline(grid_x[i], color='r')\n",
    "    plt.axhline(grid_y[i], color='r')\n",
    "\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now for each grid cross get the nearest cell (except for 0 and 512 pixels)\n",
    "all_cell_med = []\n",
    "\n",
    "# get all cell medians from stat\n",
    "for i in range(len(stat_first_iscell)):\n",
    "    all_cell_med.append(stat_first_iscell[i]['med'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute coordinates of the grid crosses\n",
    "grid_crosses = []\n",
    "for i in range(1, n_grid):\n",
    "    for j in range(1, n_grid):\n",
    "        grid_crosses.append((grid_x[i], grid_y[j]))\n",
    "\n",
    "grid_crosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ops_first['meanImg'], cmap='gray')\n",
    "for i in range(n_grid):\n",
    "    plt.axvline(grid_x[i], color='gray')\n",
    "    plt.axhline(grid_y[i], color='gray')\n",
    "for cross in grid_crosses:\n",
    "    plt.scatter(cross[0], cross[1], s=20, color='r')\n",
    "\n",
    "plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute distances between each cell and each grid cross\n",
    "distances = np.zeros((len(stat_first_iscell), len(grid_crosses)))\n",
    "for i, cell_med in enumerate(all_cell_med):\n",
    "    for j, cross in enumerate(grid_crosses):\n",
    "        distances[i, j] = np.sqrt((cell_med[0] - cross[0])**2 + (cell_med[1] - cross[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the nearest cell for each grid cross\n",
    "nearest_cells = np.argmin(distances, axis=0)\n",
    "nearest_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(ops_first['meanImg'], cmap='gray')\n",
    "\n",
    "for cross in grid_crosses:\n",
    "    plt.scatter(cross[0], cross[1], s=20, color='C0')\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "for i, cell in enumerate(nearest_cells):\n",
    "    roi = np.zeros_like(ops_first['meanImg'])\n",
    "    roi[stat_first_iscell[cell]['ypix'], stat_first_iscell[cell]['xpix']] = 1\n",
    "    plt.contour(roi, levels=[0.5], colors='C1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the original indices\n",
    "original_indices = s2p_inds_iscell[nearest_cells]\n",
    "print('Now open suite2p and try to manually track cells:')\n",
    "[print(f'{original_indices[i]}') for i in range(len(original_indices))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the same plot as above but with numbers labeled next to the roi\n",
    "plt.figure(figsize=(10, 10))\n",
    "# clip based on 99.99 percentile\n",
    "img = ops_first['meanImg'].copy()\n",
    "img[img > np.percentile(img, 99.99)] = np.percentile(img, 99.99)\n",
    "plt.imshow(img, cmap='gray')\n",
    "\n",
    "for i, cell in enumerate(nearest_cells):\n",
    "    roi = np.zeros_like(ops_first['meanImg'])\n",
    "    roi[stat_first_iscell[cell]['ypix'], stat_first_iscell[cell]['xpix']] = 1\n",
    "    plt.contour(roi, levels=[0.5], colors='C1')\n",
    "    plt.text(stat_first_iscell[cell]['med'][1]+10, stat_first_iscell[cell]['med'][0]-10, f'{original_indices[i]}', color='C1')\n",
    "    plt.scatter(grid_crosses[i][0], grid_crosses[i][1], s=20, color='C0', marker='x')\n",
    "\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting real suite2p indexes to compare it to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function(path, plane):\n",
    "\n",
    "    track_ops_dict = np.load(os.path.join(path, \"track2p\", \"track_ops.npy\"), allow_pickle=True).item()\n",
    "    track_ops = SimpleNamespace(**track_ops_dict)\n",
    "    t2p_match_mat = np.load(os.path.join(path,\"track2p\" ,f\"{plane}_match_mat.npy\"), allow_pickle=True)\n",
    "    t2p_match_mat_allday = t2p_match_mat\n",
    "    \n",
    "    if track_ops.iscell_thr is not None:\n",
    "        true_indices= np.empty((len(t2p_match_mat), len(track_ops.all_ds_path)), dtype=object)\n",
    "        \n",
    "    for j in range(len(t2p_match_mat)):\n",
    "        for (i, ds_path) in enumerate(track_ops.all_ds_path):\n",
    "            iscell = np.load(os.path.join(ds_path, 'suite2p', plane, 'iscell.npy'), allow_pickle=True)\n",
    "            if track_ops.iscell_thr is not None:\n",
    "                s2p_indexes= np.where(iscell[:,1]>track_ops.iscell_thr)[0]\n",
    "                t2p_index=t2p_match_mat[j,i]\n",
    "\n",
    "                if t2p_index is None:\n",
    "                    true_index = None\n",
    "                else:\n",
    "                    true_index=s2p_indexes[t2p_index]\n",
    "\n",
    "                true_indices[j, i] = true_index\n",
    "            else:\n",
    "                pass\n",
    "    if track_ops.iscell_thr is not None:        \n",
    "        np.save(os.path.join(path, \"true_indices.npy\"), true_indices)\n",
    "        print(\"saved\")\n",
    "    else:\n",
    "        print(\"not in locals\")\n",
    "        \n",
    "function(t2p_save_path, plane)  \n",
    "\n",
    "indices_npy= np.load(os.path.join(t2p_save_path,\"true_indices.npy\"), allow_pickle=True)\n",
    "print(indices_npy)\n",
    "print(indices_npy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(original_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index rows for cells where the first row is in original_indices\n",
    "all_s2p_idxs = []\n",
    "for idx in np.sort(original_indices):\n",
    "    idx_row = np.where(indices_npy[:,0]==idx)\n",
    "\n",
    "    if len(idx_row[0]) == 0:\n",
    "        s2p_idxs = [None]*len(track_ops.all_ds_path)\n",
    "        #print('here')\n",
    "\n",
    "    else:\n",
    "        s2p_idxs = indices_npy[idx_row]\n",
    "    all_s2p_idxs.append(s2p_idxs[0])\n",
    "\n",
    "all_s2p_idxs = np.array(all_s2p_idxs)\n",
    "\n",
    "#print(all_s2p_idxs.dtype)\n",
    "print(all_s2p_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load 'ground truth' (csv table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_matches = np.genfromtxt(os.path.join(parent_directory, 'manual_matches.csv'), delimiter=';', skip_header=1 )\n",
    "manual_matches = np.where(np.isnan(manual_matches), None, manual_matches)\n",
    "manual_matches = [[int(x) if x is not None else None for x in row] for row in manual_matches]\n",
    "manual_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(manual_matches)):\n",
    "    # Vérifier si tous les éléments sauf le premier sont \"None\"\n",
    "    if all(x == None for x in manual_matches[i][1:]):\n",
    "        manual_matches[i] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(manual_matches)):\n",
    "    # print the rows\n",
    "    print(f'track2p:     {all_s2p_idxs[i]}')\n",
    "    print(f'manual:      {manual_matches[i]}')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_all_days(all_s2p_idxs, manual_matches):\n",
    "\n",
    "    all_s2p_idxs_all_days = all_s2p_idxs.copy()\n",
    "    manual_matches_all_days = manual_matches.copy()\n",
    "\n",
    "    #IoU=1\n",
    "    for i in range(len(manual_matches)):\n",
    "        if all_s2p_idxs_all_days[i] is not None:\n",
    "            if any(x is None for x in all_s2p_idxs_all_days[i]):\n",
    "                all_s2p_idxs_all_days[i] = None\n",
    "        if manual_matches_all_days[i] is not None:\n",
    "            if any(x is None for x in manual_matches_all_days[i]):\n",
    "                manual_matches_all_days[i] = None\n",
    "\n",
    "\n",
    "       #print(f'track2p:     {all_s2p_idxs[i]}')\n",
    "       # print(f'manual:      {manual_matches[i]}')\n",
    "       #print('')\n",
    "        print(f'track2p:     {all_s2p_idxs_all_days[i]}')\n",
    "        print(f'manual:      {manual_matches_all_days[i]}')\n",
    "       # print('')\n",
    "\n",
    "\n",
    "    count_FP=0\n",
    "    count_TP=0\n",
    "    count_FN=0\n",
    "    count_IS=0\n",
    "    count_TN=0\n",
    "\n",
    "    nb_days= len(track_ops.all_ds_path)\n",
    "\n",
    "    for i in range(len(manual_matches_all_days)):\n",
    "        if all_s2p_idxs_all_days[i] is None and manual_matches_all_days[i] is None:\n",
    "             count_TN +=1\n",
    "        # if track2p detected on all days \n",
    "        elif all_s2p_idxs_all_days[i] is None and manual_matches_all_days[i] is not None:\n",
    "                count_FN +=1\n",
    "                print('FN')\n",
    "                print( all_s2p_idxs_all_days[i] )\n",
    "                print( manual_matches_all_days[i] )\n",
    "        elif all_s2p_idxs_all_days[i] is not None and manual_matches_all_days[i] is None:\n",
    "                count_FP +=1\n",
    "                print('FP')\n",
    "                print( all_s2p_idxs_all_days[i] )\n",
    "                print( manual_matches_all_days[i] )\n",
    "        elif all_s2p_idxs_all_days[i] is not None and manual_matches_all_days[i] is not None:\n",
    "            if any((x!=y) for x, y in zip(all_s2p_idxs_all_days[i], manual_matches_all_days[i])):\n",
    "                    count_IS +=1\n",
    "                    print('FP')\n",
    "                    print( all_s2p_idxs_all_days[i] )\n",
    "                    print( manual_matches_all_days[i] )\n",
    "            else:\n",
    "                    count_TP +=1\n",
    "\n",
    "    print(f'FP: {count_FP}')\n",
    "    print(f'TP: {count_TP}')\n",
    "    print(f'FN: {count_FN}')\n",
    "    print(f'IS: {count_IS}')\n",
    "\n",
    "    TP_rate= count_TP/len(manual_matches_all_days)\n",
    "    TN_rate= count_TN/len(manual_matches_all_days)\n",
    "    FP_rate= count_FP/len(manual_matches_all_days)\n",
    "    FN_rate= count_FN/len(manual_matches_all_days)\n",
    "    IS_rate= count_IS/len(manual_matches_all_days)\n",
    "\n",
    "\n",
    "    total_rate= TP_rate + TN_rate + FP_rate + FN_rate + IS_rate\n",
    "    print(f'Sum of rates: {total_rate:.2f}')\n",
    "\n",
    "    precision = count_TP/(count_TP+count_FP+count_IS)\n",
    "    recall = count_TP/(count_TP+count_FN)\n",
    "    f1 = 2*(precision*recall)/(precision+recall)\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1: {f1}')\n",
    "\n",
    "\n",
    "    filename= os.path.join(t2p_save_path,'metrics_tracking_all_days.csv')\n",
    "    with open(filename, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file ,delimiter=\";\")\n",
    "        writer.writerow(['TP', TP_rate])\n",
    "        writer.writerow(['TN', TN_rate])\n",
    "        writer.writerow(['FP', FP_rate])\n",
    "        writer.writerow(['FN', FN_rate])\n",
    "        writer.writerow(['IS', IS_rate])\n",
    "        writer.writerow(['Precision', round(precision,2)])\n",
    "        writer.writerow(['Recall', round(recall,2)])\n",
    "        writer.writerow(['F1', round(f1,2)])\n",
    "    print(f\"CSV file '{filename}' created successfully.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics_all_days(all_s2p_idxs, manual_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions to get metrics (FP,FN,F1...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_pairs(manual_matches_elements,all_s2p_idxs_elements):\n",
    "    \n",
    "    TP_indexes=[]\n",
    "    FP_indexes=[]\n",
    "    TN_indexes=[]\n",
    "    FN_indexes=[]\n",
    "    IS_indexes=[]\n",
    "\n",
    "    for i in range(len(manual_matches_elements)):\n",
    "\n",
    "        if  all_s2p_idxs_elements[i][1] == manual_matches_elements[i][1] and all_s2p_idxs_elements[i][1] is not None:\n",
    "            TP_indexes.append(i)\n",
    "\n",
    "        elif all_s2p_idxs_elements[i][1] == manual_matches_elements[i][1] and all_s2p_idxs_elements[i][1] is None: \n",
    "            TN_indexes.append(i)\n",
    "    \n",
    "        elif manual_matches_elements[i][1] is None and all_s2p_idxs_elements[i][1] is not None:\n",
    "            FP_indexes.append(i)\n",
    "    \n",
    "        elif all_s2p_idxs_elements[i][1] is None and manual_matches_elements[i][1] is not None:\n",
    "            FN_indexes.append(i)\n",
    "\n",
    "        elif all_s2p_idxs_elements[i][1] != manual_matches_elements[i][1] :\n",
    "            IS_indexes.append(i)\n",
    "            #FP_indexes.append(i)\n",
    "\n",
    "    TP_rate= len(TP_indexes)/len(manual_matches_elements)\n",
    "    TN_rate= len(TN_indexes)/len(manual_matches_elements)\n",
    "    FP_rate= len(FP_indexes)/len(manual_matches_elements)\n",
    "    FN_rate= len(FN_indexes)/len(manual_matches_elements)\n",
    "    IS_rate= len(IS_indexes)/len(manual_matches_elements)\n",
    "\n",
    "\n",
    "    print(f'TP: {len(TP_indexes)}   {TP_rate:.2f} \\n')\n",
    "    print(f'TN: {len(TN_indexes)}   {TN_rate:.2f} \\n')\n",
    "    print(f'FP: {len(FP_indexes)}   {FP_rate:.2f} \\n')\n",
    "    print(f'FN: {len(FN_indexes)}   {FN_rate:.2f} \\n')\n",
    "    print(f'IS: {len(IS_indexes)}   {IS_rate:.2f} \\n')\n",
    "\n",
    "    correctly_assign_link_rate = TP_rate + TN_rate\n",
    "    incorrect_assign_link_rate = FP_rate + FN_rate + IS_rate\n",
    "\n",
    "    print(f'correctly_assign_link_rate: {correctly_assign_link_rate:.2f}')\n",
    "    print(f'incorrect_assign_link_rate: {incorrect_assign_link_rate:.2f}')\n",
    "\n",
    "    total_rate= TP_rate + TN_rate + FP_rate + FN_rate + IS_rate\n",
    "    print(f'Sum of rates: {total_rate:.2f}')\n",
    "\n",
    "    precision= len(TP_indexes)/(len(TP_indexes) + (len(FP_indexes)+ len(IS_indexes)))\n",
    "    recall= len(TP_indexes)/(len(TP_indexes) + len(FN_indexes))\n",
    "    f1_score= (2*(precision*recall))/(precision+recall)\n",
    "\n",
    "    return TP_rate, TN_rate, FP_rate, FN_rate, IS_rate, correctly_assign_link_rate, incorrect_assign_link_rate, precision, recall, f1_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_metrics_to_csv(TP_rate_all_days, TN_rate_all_days, FP_rate_all_days, FN_rate_all_days, IS_rate_all_days,correctly_assign_link_rate_all_days, incorrect_assign_link_rate_all_days, precision_all_days, recall_all_days, f1_score_all_days, option):\n",
    "        \n",
    "    if option ==1 :\n",
    "        row_names = ['TP_rate', 'TN_rate', 'FP_rate', 'FN_rate', 'IS_rate', 'correctly_assign_link_rate', 'incorrect_assign_link_rate', 'precision', 'recall', 'f1_score']\n",
    "        column_names = ['P9', 'P10', 'P11', 'P12', 'P13', 'P14']\n",
    "\n",
    "                        # Créer une liste de listes pour les données\n",
    "        data = [\n",
    "                TP_rate_all_days,\n",
    "                TN_rate_all_days,\n",
    "                FP_rate_all_days,\n",
    "                FN_rate_all_days,\n",
    "                IS_rate_all_days,\n",
    "                correctly_assign_link_rate_all_days,\n",
    "                incorrect_assign_link_rate_all_days,\n",
    "                precision_all_days,\n",
    "                recall_all_days,\n",
    "                f1_score_all_days\n",
    "                ]\n",
    "            \n",
    "\n",
    "            # Ajouter les noms des lignes\n",
    "        data_with_row_names = [[row_names[i]] + [round(value, 2) for value in data[i]] for i in range(len(row_names))]\n",
    "\n",
    "            \n",
    "        filename= os.path.join(t2p_save_path,'metrics.csv')\n",
    "            # Écrire les données dans un fichier CSV\n",
    "        with open(filename, 'w', newline='') as file:\n",
    "                writer = csv.writer(file, delimiter=';')\n",
    "                writer.writerow([''] + column_names)  # Écrire les noms des colonnes\n",
    "                writer.writerows(data_with_row_names)  # Écrire les données avec les noms des lignes\n",
    "\n",
    "            \n",
    "        print(f\"CSV file '{filename}' created successfully.\")\n",
    "    \n",
    "    if option ==2: \n",
    "                \n",
    "        row_names = ['TP_rate', 'TN_rate', 'FP_rate', 'FN_rate', 'IS_rate', 'correctly_assign_link_rate', 'incorrect_assign_link_rate', 'precision', 'recall', 'f1_score']\n",
    "        column_names = [days[index_of_second_day]]\n",
    "\n",
    "                        # Créer une liste de listes pour les données\n",
    "        data = [\n",
    "                TP_rate_all_days,\n",
    "                TN_rate_all_days,\n",
    "                FP_rate_all_days,\n",
    "                FN_rate_all_days,\n",
    "                IS_rate_all_days,\n",
    "                correctly_assign_link_rate_all_days,\n",
    "                incorrect_assign_link_rate_all_days,\n",
    "                precision_all_days,\n",
    "                recall_all_days,\n",
    "                f1_score_all_days\n",
    "                ]\n",
    "            \n",
    "                # Ajouter les noms des lignes\n",
    "        data_with_row_names = [[row_names[i]] + [round(value, 2) for value in data[i]] for i in range(len(row_names))]\n",
    "\n",
    "        filename= os.path.join(t2p_save_path,'metrics.csv')\n",
    "                # Écrire les données dans un fichier CSV\n",
    "        with open(filename, 'w', newline='') as file:\n",
    "            writer = csv.writer(file, delimiter=';')\n",
    "            writer.writerow([''] + column_names)  # Écrire les noms des colonnes\n",
    "            writer.writerows(data_with_row_names)  # Écrire les données avec les noms des lignes\n",
    "        print(f\"CSV file '{filename}' created successfully.\")\n",
    "\n",
    "    if option ==3:\n",
    "         \n",
    "        row_names = ['TP_rate', 'TN_rate', 'FP_rate', 'FN_rate', 'IS_rate', 'correctly_assign_link_rate', 'incorrect_assign_link_rate', 'precision', 'recall', 'f1_score']\n",
    "        column_names = [days[index_of_second_day], days[index_of_third_day]]\n",
    "\n",
    "                        # Créer une liste de listes pour les données\n",
    "        data = [\n",
    "                TP_rate_all_days,\n",
    "                TN_rate_all_days,\n",
    "                FP_rate_all_days,\n",
    "                FN_rate_all_days,\n",
    "                IS_rate_all_days,\n",
    "                correctly_assign_link_rate_all_days,\n",
    "                incorrect_assign_link_rate_all_days,\n",
    "                precision_all_days,\n",
    "                recall_all_days,\n",
    "                f1_score_all_days\n",
    "                ]\n",
    "            \n",
    "                # Ajouter les noms des lignes\n",
    "        data_with_row_names = [[row_names[i]] + [round(value, 2) for value in data[i]] for i in range(len(row_names))]\n",
    "\n",
    "        filename= os.path.join(t2p_save_path,'metrics.csv')\n",
    "        \n",
    "                # Écrire les données dans un fichier CSV\n",
    "        with open(filename, 'w', newline='') as file:\n",
    "            writer = csv.writer(file, delimiter=';')\n",
    "            writer.writerow([''] + column_names)\n",
    "            writer.writerows(data_with_row_names)  # Écrire les données avec les noms des lignes\n",
    "        print(f\"CSV file '{filename}' created successfully.\")\n",
    "\n",
    "    \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_day_evaluation(manual_matches,all_s2p_idxs, option):\n",
    "\n",
    "    corr_assign_link_all_days = []\n",
    "    TP_rate_all_days = []\n",
    "    TN_rate_all_days = []\n",
    "    FP_rate_all_days = []\n",
    "    FN_rate_all_days = []\n",
    "    IS_rate_all_days = []\n",
    "    incorr_assign_link_all_days = []\n",
    "    precision_all_days = []\n",
    "    recall_all_days = []\n",
    "    f1_score_all_days = []\n",
    "\n",
    "    if option==1: \n",
    "        for day in range(1 , t2p_match_mat.shape[1]): \n",
    "\n",
    "            print(f\"Day {day}\")\n",
    "\n",
    "            # obtain pairs (P8-P9..P14)\n",
    "            manual_matches_elements = [(sublist[0], sublist[day]) if sublist is not None and len(sublist) > day else (None, None) for sublist in manual_matches]\n",
    "            all_s2p_idxs_elements = [(sublist[0], sublist[day]) if sublist is not None and len(sublist) > day else (None, None) for sublist in all_s2p_idxs]\n",
    "\n",
    "            print(\"manual_matches_elements:\", manual_matches_elements)\n",
    "            print(\"all_s2p_idxs_elements:\", all_s2p_idxs_elements)\n",
    "\n",
    "\n",
    "            TP_rate, TN_rate, FP_rate, FN_rate, IS_rate, correctly_assign_link_rate, incorrect_assign_link_rate, precision, recall, f1_score = calculate_metrics_pairs(manual_matches_elements,all_s2p_idxs_elements)\n",
    "\n",
    "            corr_assign_link_all_days.append(correctly_assign_link_rate)\n",
    "            TP_rate_all_days.append(TP_rate)\n",
    "            TN_rate_all_days.append(TN_rate)\n",
    "            FP_rate_all_days.append(FP_rate)\n",
    "            FN_rate_all_days.append(FN_rate)\n",
    "            IS_rate_all_days.append(IS_rate)\n",
    "            incorr_assign_link_all_days.append(incorrect_assign_link_rate)\n",
    "            precision_all_days.append(precision)\n",
    "            recall_all_days.append(recall)\n",
    "            f1_score_all_days.append(f1_score)\n",
    "        \n",
    "        write_metrics_to_csv(TP_rate_all_days, TN_rate_all_days, FP_rate_all_days, FN_rate_all_days, IS_rate_all_days, corr_assign_link_all_days, incorr_assign_link_all_days, precision_all_days, recall_all_days, f1_score_all_days, option)\n",
    "            \n",
    "            \n",
    "    if option==2: \n",
    "                manual_matches_elements = [(sublist[0], sublist[index_of_second_day]) if sublist is not None  else (None, None) for sublist in manual_matches]\n",
    "                # Récupérer l'élément 1 et l'élément 5 de chaque sous-liste dans all_s2p_idxs_days\n",
    "                all_s2p_idxs_elements = [(sublist[0], sublist[1]) if sublist is not None else (None, None) for sublist in all_s2p_idxs]\n",
    "                print(\"manual_matches_elements:\", manual_matches_elements)\n",
    "                print(\"all_s2p_idxs_elements:\", all_s2p_idxs_elements)\n",
    "\n",
    "\n",
    "                TP_rate, TN_rate, FP_rate, FN_rate, IS_rate, correctly_assign_link_rate, incorrect_assign_link_rate, precision, recall, f1_score = calculate_metrics_pairs(manual_matches_elements,all_s2p_idxs_elements)\n",
    "\n",
    "                corr_assign_link_all_days.append(correctly_assign_link_rate)\n",
    "                TP_rate_all_days.append(TP_rate)\n",
    "                TN_rate_all_days.append(TN_rate)\n",
    "                FP_rate_all_days.append(FP_rate)\n",
    "                FN_rate_all_days.append(FN_rate)\n",
    "                IS_rate_all_days.append(IS_rate)\n",
    "                incorr_assign_link_all_days.append(incorrect_assign_link_rate)\n",
    "                precision_all_days.append(precision)\n",
    "                recall_all_days.append(recall)\n",
    "                f1_score_all_days.append(f1_score)\n",
    "\n",
    "                write_metrics_to_csv(TP_rate_all_days, TN_rate_all_days, FP_rate_all_days, FN_rate_all_days, IS_rate_all_days, corr_assign_link_all_days, incorr_assign_link_all_days, precision_all_days, recall_all_days, f1_score_all_days, option)\n",
    "\n",
    "\n",
    "    if option==3:\n",
    "                manual_matches_elements = [(sublist[0], sublist[index_of_second_day]) if sublist is not None  else (None, None) for sublist in manual_matches]\n",
    "                # Récupérer l'élément 1 et l'élément 5 de chaque sous-liste dans all_s2p_idxs_days\n",
    "                all_s2p_idxs_elements = [(sublist[0], sublist[1]) if sublist is not None else (None, None) for sublist in all_s2p_idxs]\n",
    "\n",
    "                print(\"manual_matches_elements:\", manual_matches_elements)\n",
    "                print(\"all_s2p_idxs_elements:\", all_s2p_idxs_elements)\n",
    "\n",
    "\n",
    "                TP_rate, TN_rate, FP_rate, FN_rate, IS_rate, correctly_assign_link_rate, incorrect_assign_link_rate, precision, recall, f1_score = calculate_metrics_pairs(manual_matches_elements,all_s2p_idxs_elements)\n",
    "\n",
    "                corr_assign_link_all_days.append(correctly_assign_link_rate)\n",
    "                TP_rate_all_days.append(TP_rate)\n",
    "                TN_rate_all_days.append(TN_rate)\n",
    "                FP_rate_all_days.append(FP_rate)\n",
    "                FN_rate_all_days.append(FN_rate)\n",
    "                IS_rate_all_days.append(IS_rate)\n",
    "                incorr_assign_link_all_days.append(incorrect_assign_link_rate)\n",
    "                precision_all_days.append(precision)\n",
    "                recall_all_days.append(recall)\n",
    "                f1_score_all_days.append(f1_score)\n",
    "            \n",
    "\n",
    "                manual_matches_elements = [(sublist[0], sublist[index_of_third_day]) if sublist is not None  else (None, None) for sublist in manual_matches]\n",
    "                # Récupérer l'élément 1 et l'élément 5 de chaque sous-liste dans all_s2p_idxs_days\n",
    "                all_s2p_idxs_elements = [(sublist[0], sublist[2]) if sublist is not None else (None, None) for sublist in all_s2p_idxs]\n",
    "\n",
    "\n",
    "                print(\"manual_matches_elements:\", manual_matches_elements)\n",
    "                print(\"all_s2p_idxs_elements:\", all_s2p_idxs_elements)\n",
    "          \n",
    "\n",
    "                TP_rate, TN_rate, FP_rate, FN_rate, IS_rate, correctly_assign_link_rate, incorrect_assign_link_rate, precision, recall, f1_score = calculate_metrics_pairs(manual_matches_elements,all_s2p_idxs_elements)\n",
    "\n",
    "                corr_assign_link_all_days.append(correctly_assign_link_rate)\n",
    "                TP_rate_all_days.append(TP_rate)\n",
    "                TN_rate_all_days.append(TN_rate)\n",
    "                FP_rate_all_days.append(FP_rate)\n",
    "                FN_rate_all_days.append(FN_rate)\n",
    "                IS_rate_all_days.append(IS_rate)\n",
    "                incorr_assign_link_all_days.append(incorrect_assign_link_rate)\n",
    "                precision_all_days.append(precision)\n",
    "                recall_all_days.append(recall)\n",
    "                f1_score_all_days.append(f1_score)\n",
    "\n",
    "                write_metrics_to_csv(TP_rate_all_days, TN_rate_all_days, FP_rate_all_days, FN_rate_all_days, IS_rate_all_days, corr_assign_link_all_days, incorr_assign_link_all_days, precision_all_days, recall_all_days, f1_score_all_days, option)\n",
    "            \n",
    "\n",
    "    return {\n",
    "            'TP_rate': TP_rate_all_days,\n",
    "            'TN_rate': TN_rate_all_days,\n",
    "            'FP_rate': FP_rate_all_days,\n",
    "            'FN_rate': FN_rate_all_days,\n",
    "            'IS_rate': IS_rate_all_days,\n",
    "            'correct_links_rate': corr_assign_link_all_days,\n",
    "            'incorrect_links_rate': incorr_assign_link_all_days,\n",
    "            'precision': precision_all_days,\n",
    "            'recall': recall_all_days,\n",
    "            'f1_score': f1_score_all_days\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = multi_day_evaluation(manual_matches, all_s2p_idxs, option)\n",
    "#metrics[path] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIGURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pairwise evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all metrics.csv tables\n",
    "def load_metrics_csv(path):\n",
    "    metrics = pd.read_csv(path, delimiter=';')\n",
    "    return metrics\n",
    "\n",
    "#Pairwise evaluation (P8-P9, P9-P10, P10-P11, P11-P12, P12-P13, P13-P14)\n",
    "\n",
    "mtc_t2p_aff_chan0_all= load_metrics_csv(os.path.join(parent_directory, 't2p_aff_chan0_all_', 'metrics.csv'))\n",
    "mtc_t2p_aff_chan1_all= load_metrics_csv(os.path.join(parent_directory, 't2p_affine_chan1_all_', 'metrics.csv'))\n",
    "mtc_t2p_rig_chan1_all= load_metrics_csv(os.path.join(parent_directory, 't2p_rigid_chan1_all_', 'metrics.csv'))\n",
    "mtc_t2p_P8_9= load_metrics_csv(os.path.join(parent_directory, 't2p_P8_9', 'metrics.csv'))\n",
    "mtc_t2p_P8_10= load_metrics_csv(os.path.join(parent_directory, 't2p_P8_10', 'metrics.csv'))\n",
    "mtc_t2p_P8_11= load_metrics_csv(os.path.join(parent_directory, 't2p_P8_11', 'metrics.csv'))\n",
    "mtc_t2p_P8_12= load_metrics_csv(os.path.join(parent_directory, 't2p_P8_12', 'metrics.csv'))\n",
    "mtc_t2p_P8_13= load_metrics_csv(os.path.join(parent_directory, 't2p_P8_13', 'metrics.csv'))\n",
    "mtc_t2p_P8_14= load_metrics_csv(os.path.join(parent_directory, 't2p_P8_14', 'metrics.csv'))\n",
    "\n",
    "mtc_t2p_P8_P9_P14= load_metrics_csv(os.path.join(parent_directory, 't2p_P8_9_14', 'metrics.csv'))\n",
    "mtc_t2p_P8_P10_P14= load_metrics_csv(os.path.join(parent_directory, 't2p_P8_10_14', 'metrics.csv'))\n",
    "mtc_t2p_P8_P11_P14= load_metrics_csv(os.path.join(parent_directory, 't2p_P8_11_14', 'metrics.csv'))\n",
    "mtc_t2p_P8_P12_P14= load_metrics_csv(os.path.join(parent_directory, 't2p_P8_12_14', 'metrics.csv'))\n",
    "mtc_t2p_P8_P13_P14= load_metrics_csv(os.path.join(parent_directory, 't2p_P8_13_14', 'metrics.csv'))\n",
    "\n",
    "print(mtc_t2p_aff_chan0_all)\n",
    "# Merge the DataFrames on the first column\n",
    "mtc_t2p_aff_chan1_pairs = mtc_t2p_P8_9\n",
    "for df in [mtc_t2p_P8_10, mtc_t2p_P8_11, mtc_t2p_P8_12, mtc_t2p_P8_13, mtc_t2p_P8_14]:\n",
    "    mtc_t2p_aff_chan1_pairs = pd.merge(mtc_t2p_aff_chan1_pairs, df, on=mtc_t2p_aff_chan1_pairs.columns[0], suffixes=('', '_'+df.columns[1]))\n",
    "print(mtc_t2p_aff_chan1_pairs)\n",
    "\n",
    "output_csv_path = os.path.join(parent_directory, 'mtc_t2p_aff_chan1_pairs.csv')\n",
    "mtc_t2p_aff_chan1_pairs.to_csv(output_csv_path, index=False, sep=';')\n",
    "print(f\"DataFrame saved to {output_csv_path}\")\n",
    "\n",
    "# Initialiser le DataFrame avec la première colonne et la colonne P14 du premier DataFrame\n",
    "mtc_t2_aff_chan1_3days = mtc_t2p_P8_P9_P14[[mtc_t2p_P8_P9_P14.columns[0], 'P14']].rename(columns={'P14': 'P9'})\n",
    "\n",
    "# Fusionner les DataFrames en utilisant la première colonne comme clé et la colonne P14 pour les valeurs\n",
    "for df, day in zip([mtc_t2p_P8_P10_P14, mtc_t2p_P8_P11_P14, mtc_t2p_P8_P12_P14, mtc_t2p_P8_P13_P14], ['P10', 'P11', 'P12', 'P13']):\n",
    "    mtc_t2_aff_chan1_3days = pd.merge(mtc_t2_aff_chan1_3days, df[[df.columns[0], 'P14']].rename(columns={'P14': day}), on=mtc_t2_aff_chan1_3days.columns[0])\n",
    "\n",
    "print(mtc_t2_aff_chan1_3days)\n",
    "\n",
    "output_csv_path = os.path.join(parent_directory, 'mtc_t2p_aff_chan1_3days.csv')\n",
    "mtc_t2_aff_chan1_3days.to_csv(output_csv_path, index=False, sep=';')\n",
    "print(f\"DataFrame saved to {output_csv_path}\")\n",
    "\n",
    "#dfs= [mtc_t2p_aff_chan0_all,mtc_t2p_aff_chan1_all,mtc_t2p_rig_chan1_all,mtc_t2p_aff_chan0_pairs]\n",
    "dfs_dict = {\n",
    "    'Aff_tdT_AD': mtc_t2p_aff_chan1_all,\n",
    "    'Aff_GCamp_AD': mtc_t2p_aff_chan0_all,\n",
    "    'Rig_tdT_AD': mtc_t2p_rig_chan1_all,\n",
    "    'Aff_tdT_PW': mtc_t2p_aff_chan1_pairs,\n",
    "    'Aff_tdT_3days': mtc_t2_aff_chan1_3days\n",
    "}\n",
    "\n",
    "['Aff_Int_AD', 'Aff_Act_AD', 'Rig_Int_AD', 'Aff_Int_PW']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.colors import to_rgba\n",
    "cmap_list = ['C0', 'C1', 'C2', 'C3']\n",
    "from matplotlib.colors import to_rgba\n",
    "cmap_list_bis = ['C0', 'C1', 'C2', 'C3','C4']\n",
    "\n",
    "dfs_colors = [to_rgba(cmap) for cmap in cmap_list]\n",
    "\n",
    "def generate_transparency_gradient(num_colors, idx):\n",
    "    base_color = dfs_colors\n",
    "    gradient = []\n",
    "    for i in range(num_colors):\n",
    "        alpha = 0.2 + 0.9 * (i / (num_colors - 1)) \n",
    "        color_with_alpha = np.append(base_color[:3], alpha)  # Ajouter l'alpha à la couleur de base\n",
    "        gradient.append(color_with_alpha)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib.colors import to_rgba\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cmap_list = ['C0', 'C1', 'C2', 'C3', 'C4']\n",
    "dfs_colors = [to_rgba(cmap) for cmap in cmap_list]\n",
    "\n",
    "def generate_transparency_gradient(num_colors, idx):\n",
    "    base_color = np.array(dfs_colors[idx])\n",
    "    gradient = []\n",
    "    for i in range(num_colors):\n",
    "        alpha = 0.3 + 0.7 * (i / (num_colors - 1))   \n",
    "        color_with_alpha = np.append(base_color[:3], alpha)  # Ajouter l'alpha à la couleur de base\n",
    "        gradient.append(color_with_alpha)\n",
    "    return gradient\n",
    "\n",
    "for idx, (data, base_color) in enumerate(zip(dfs_dict, dfs_colors)):\n",
    "    print(data)\n",
    "    df = dfs_dict[data]\n",
    "    print(df)\n",
    "    f1_pcs_rcl = df[df[df.columns[0]].isin(['precision', 'recall', 'f1_score'])]\n",
    "    rates = df[df[df.columns[0]].isin(['TP_rate', 'TN_rate', 'FP_rate','FN_rate','IS_rate'])]\n",
    "    \n",
    "    # Générer un dégradé de couleurs pour les tracés\n",
    "    color_gradient = generate_transparency_gradient(len(f1_pcs_rcl), idx=idx)\n",
    "    \n",
    "    # Plot f1, precision, recall\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    for (index, row), color in zip(f1_pcs_rcl.iterrows(), color_gradient):\n",
    "        plt.plot(df.columns[1:], row[1:], marker='o', markersize=4, label=row[0], color=color)\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.yticks([0, 0.5, 1])\n",
    "        plt.xlabel('Age (postnatal days)')\n",
    "        ax = plt.gca()\n",
    "        x_ticks = range(len(df.columns) - 1)\n",
    "        ax.set_xticks(x_ticks)\n",
    "        x_tick_labels = [''] * len(x_ticks)\n",
    "        x_tick_labels[0] = df.columns[1]\n",
    "        x_tick_labels[-1] = df.columns[-1]\n",
    "        ax.set_xticklabels(x_tick_labels)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  # Placer la légende à droite\n",
    "    plt.show()\n",
    "    \n",
    "    # Générer un dégradé de couleurs pour les tracés\n",
    "    color_gradient = generate_transparency_gradient(len(rates), idx=idx)\n",
    "    \n",
    "    # Plot rates\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    for (index, row), color in zip(rates.iterrows(), cmap_list_bis):\n",
    "        plt.plot(df.columns[1:], row[1:], marker='o', markersize=4, label=row[0], color=color)\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.yticks([0, 0.5, 1])\n",
    "        plt.xlabel('Age (postnatal days)')\n",
    "        ax = plt.gca()\n",
    "        x_ticks = range(len(df.columns) - 1)\n",
    "        ax.set_xticks(x_ticks)\n",
    "        x_tick_labels = [''] * len(x_ticks)\n",
    "        x_tick_labels[0] = df.columns[1]\n",
    "        x_tick_labels[-1] = df.columns[-1]\n",
    "        ax.set_xticklabels(x_tick_labels)\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))  # Placer la légende à droite\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = ['Aff_Int_AD', 'Aff_Act_AD', 'Rig_Int_AD', 'Aff_Int_PW']\n",
    "\n",
    "plt.figure(figsize=(3, 3))\n",
    "for color, (data, df) in zip(cmap_list, dfs_dict.items()):\n",
    "    f1_score_row = df[df[df.columns[0]] == 'f1_score']\n",
    "    print(data)\n",
    "    print(f1_score_row)\n",
    "    for index, row in f1_score_row.iterrows():\n",
    "        plt.plot(df.columns[1:], row[1:], marker='o', markersize=4, label=f'{data}', color=color)\n",
    "        \n",
    "\n",
    "plt.ylim(0, 1.05)\n",
    "plt.yticks([0, 0.5, 1])\n",
    "plt.ylabel('F1 Score')\n",
    "\n",
    "ax = plt.gca()\n",
    "x_ticks = range(len(df.columns) - 1)\n",
    "ax.set_xticks(x_ticks)\n",
    "x_tick_labels = [''] * len(x_ticks)\n",
    "x_tick_labels[0] = df.columns[1]\n",
    "x_tick_labels[-1] = df.columns[-1]\n",
    "ax.set_xticklabels(x_tick_labels)\n",
    "\n",
    "\n",
    "plt.xlabel('Age (postnatal days)')\n",
    "plt.legend()\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All day evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mtc_t2p_aff_chan0_all_track= pd.read_csv(os.path.join(parent_directory, 't2p_aff_chan0_all_', 'metrics_tracking_all_days.csv'),  names=['Metric', 'Value'], delimiter=';')\n",
    "mtc_t2p_aff_chan1_all_track= pd.read_csv(os.path.join(parent_directory, 't2p_affine_chan1_all_', 'metrics_tracking_all_days.csv'),  names=['Metric', 'Value'], delimiter=';')\n",
    "mtc_t2p_rig_chan1_all_track= pd.read_csv(os.path.join(parent_directory, 't2p_rigid_chan1_all_', 'metrics_tracking_all_days.csv'),  names=['Metric', 'Value'], delimiter=';')\n",
    "\n",
    "print(mtc_t2p_aff_chan0_all_track)\n",
    "print(mtc_t2p_aff_chan1_all_track)\n",
    "print(mtc_t2p_rig_chan1_all_track)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_precision = []\n",
    "all_f1 = []\n",
    "all_recall = []\n",
    "\n",
    "for data in [mtc_t2p_aff_chan1_all_track, mtc_t2p_aff_chan0_all_track, mtc_t2p_rig_chan1_all_track]:\n",
    "    precision = data.loc[data['Metric'] == 'Precision', 'Value'].values[0]\n",
    "    recall = data.loc[data['Metric'] == 'Recall', 'Value'].values[0]\n",
    "    f1 = data.loc[data['Metric'] == 'F1', 'Value'].values[0]\n",
    "    all_precision.append(precision)\n",
    "    all_recall.append(recall)\n",
    "    all_f1.append(f1)\n",
    "\n",
    "print(all_precision)\n",
    "print(all_recall)\n",
    "print(all_f1)\n",
    "\n",
    "labels = ['Aff_tdT_AD', 'Aff_GCamp_AD', 'Rig_tdT_AD']\n",
    "\n",
    "\n",
    "# Création du barplot pour les valeurs de F1 avec légende\n",
    "plt.figure(figsize=(3, 3))\n",
    "for i, (label, f1) in enumerate(zip(labels, all_f1)):\n",
    "    plt.bar(i, f1, color=['C0', 'C1', 'C2'][i], width=0.5, label=label)\n",
    "plt.ylabel('F1 Score')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.yticks([0, 0.5, 1])\n",
    "plt.xticks(ticks=range(len(labels)), labels=[''] * len(labels))\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "plt.legend()\n",
    "plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "plt.show()\n",
    "\n",
    "# Création du barplot pour les valeurs de précision avec légende\n",
    "plt.figure(figsize=(3, 3))\n",
    "for i, (label, precision) in enumerate(zip(labels, all_precision)):\n",
    "    plt.bar(i, precision, color=['C0', 'C1', 'C2'][i], width=0.5, label=label)\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.yticks([0, 0.5, 1])\n",
    "plt.xticks(ticks=range(len(labels)), labels=[''] * len(labels))\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Création du barplot pour les valeurs de rappel avec légende\n",
    "plt.figure(figsize=(3, 3))\n",
    "for i, (label, recall) in enumerate(zip(labels, all_recall)):\n",
    "    plt.bar(i, recall, color=['C0', 'C1', 'C2'][i], width=0.5, label=label)\n",
    "plt.ylabel('Recall')\n",
    "plt.ylim(0, 1.05)\n",
    "plt.yticks([0, 0.5, 1])\n",
    "plt.xticks(ticks=range(len(labels)), labels=[''] * len(labels))\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.tick_params(axis='x', which='both', bottom=False, top=False)\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "track2p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
